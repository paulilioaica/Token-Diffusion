{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_transformer import TokenDiffusionModel\n",
    "import urllib.request\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "# urllib.request.urlretrieve(url, 'input.txt')\n",
    "\n",
    "\n",
    "# Read the text file\n",
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = [x for x in  text.lower().split() if len(x) > 0]\n",
    "\n",
    "unqiue_words = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenDiffusionModel(\n",
      "  (embedding): Embedding(23642, 128)\n",
      "  (transformer_decoder_layer): TransformerDecoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (multihead_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    (dropout3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=128, out_features=23642, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = len(unqiue_words) + 1  # Size of the vocabulary plus mask\n",
    "embedding_dim = 128  # Size of embeddings (e.g., BERT-like model)\n",
    "hidden_dim = 128  # Transformer hidden layer size\n",
    "num_iterations = 100  # Number of iterative refinement steps\n",
    "max_seq_len = 64  # Maximum sequence length\n",
    "\n",
    "# Instantiate the model\n",
    "model = TokenDiffusionModel(vocab_size, embedding_dim, hidden_dim, num_iterations, max_seq_len).to(device)\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dict of words to index\n",
    "\n",
    "word_to_index = {word: i for i, word in enumerate(unqiue_words)}\n",
    "\n",
    "# create a list of indices\n",
    "\n",
    "indices = [word_to_index[word] for word in words]\n",
    "\n",
    "\n",
    "sequences = [indices[i:i+max_seq_len] for i in range(len(indices)-max_seq_len)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_id = 23641\n",
    "\n",
    "def add_noise_to_sequence(sequence, noise_type='mask', mask_prob=0.25):\n",
    "    noisy_sequence = list(sequence)\n",
    "    seq_len = len(sequence)\n",
    "\n",
    "    #save a mask and target token id\n",
    "    target = [-1] * seq_len\n",
    "    \n",
    "    if noise_type == 'mask':\n",
    "        for i in range(seq_len):\n",
    "            if random.random() < mask_prob:\n",
    "                # Replace with random token\n",
    "                target[i] = noisy_sequence[i]\n",
    "                noisy_sequence[i] = mask_id\n",
    "    \n",
    "    # elif noise_type == 'shuffle':\n",
    "    #     indices = list(range(seq_len))\n",
    "    #     random.shuffle(indices)\n",
    "    #     noisy_sequence = [sequence[i] for i in indices]\n",
    "    \n",
    "    # elif noise_type == 'replace':\n",
    "    #     for i in range(seq_len):\n",
    "    #         if random.random() < mask_prob:\n",
    "    #             # Replace with random token\n",
    "    #             noisy_sequence[i] = random.randint(0, vocab_size - 1)\n",
    "    \n",
    "    return torch.tensor(noisy_sequence), torch.tensor(target)\n",
    "\n",
    "targets = []\n",
    "noisy_sequences = []\n",
    "\n",
    "for i in range(len(sequences)):\n",
    "    noisy_sequence, target = add_noise_to_sequence(sequences[i], noise_type='mask', mask_prob=0.25)\n",
    "    targets.append(target)\n",
    "    noisy_sequences.append(noisy_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make these into batches\n",
    "batch_size = 32\n",
    "\n",
    "batches = []\n",
    "for i in range(0, len(noisy_sequences), batch_size):\n",
    "    batch = noisy_sequences[i:i+batch_size]\n",
    "    target = targets[i:i+batch_size]\n",
    "    batches.append((torch.vstack(batch).to(device), torch.vstack(target).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64])\n",
      "torch.Size([32, 64, 23642])\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.30741024017334\n",
      "9.917768478393555\n",
      "9.827102661132812\n",
      "9.762602806091309\n"
     ]
    }
   ],
   "source": [
    "#start training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for sequence, target in batches:\n",
    "    optimizer.zero_grad()\n",
    "    output = model(sequence)\n",
    "    loss = criterion(output.view(-1, vocab_size), target.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
