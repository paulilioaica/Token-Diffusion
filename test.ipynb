{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_transformer import TokenDiffusionModel\n",
    "from dataset import TinyShakespeareDataset\n",
    "import urllib.request\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TinyShakespeareDataset('input.txt', seq_len=32)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenDiffusionModel(\n",
      "  (embedding): Embedding(65, 32)\n",
      "  (transformer_decoder_layer): TransformerDecoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (multihead_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    (dropout3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=32, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = dataset.vocab_size  # Size of the vocabulary plus mask\n",
    "embedding_dim = 32  # Size of embeddings (e.g., BERT-like model)\n",
    "hidden_dim = 32  # Transformer hidden layer size\n",
    "num_iterations = 200  # Number of iterative refinement steps\n",
    "max_seq_len = 32  # Maximum sequence length\n",
    "num_layers = 1\n",
    "nhead = 4\n",
    "# self, vocab_size, embedding_dim, hidden_dim, num_layers, nhead, max_seq_len, dropout=0.1\n",
    "# Instantiate the model\n",
    "model = TokenDiffusionModel(vocab_size, embedding_dim, hidden_dim, num_layers, nhead, num_iterations, max_seq_len).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ..\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [0], Loss: 4.4782\n",
      "Epoch [1/10], Batch [10], Loss: 4.3291\n",
      "Epoch [1/10], Batch [20], Loss: 4.1472\n",
      "Epoch [1/10], Batch [30], Loss: 4.0476\n",
      "Epoch [1/10], Batch [40], Loss: 4.0059\n",
      "Epoch [1/10], Batch [50], Loss: 3.9395\n",
      "Epoch [1/10], Batch [60], Loss: 3.8836\n",
      "Epoch [1/10], Batch [70], Loss: 3.8634\n",
      "Epoch [1/10], Batch [80], Loss: 3.8099\n",
      "Epoch [1/10], Batch [90], Loss: 3.7815\n",
      "Epoch [1/10], Batch [100], Loss: 3.7451\n",
      "Epoch [1/10], Batch [110], Loss: 3.6970\n",
      "Epoch [1/10], Batch [120], Loss: 3.6625\n",
      "Epoch [1/10], Batch [130], Loss: 3.7118\n",
      "Epoch [1/10], Batch [140], Loss: 3.7058\n",
      "Epoch [1/10], Batch [150], Loss: 3.6446\n",
      "Epoch [1/10], Batch [160], Loss: 3.6144\n",
      "Epoch [1/10], Batch [170], Loss: 3.6059\n",
      "Epoch [1/10], Batch [180], Loss: 3.5518\n",
      "Epoch [1/10], Batch [190], Loss: 3.5723\n",
      "Epoch [1/10], Batch [200], Loss: 3.5181\n",
      "Epoch [1/10], Batch [210], Loss: 3.5595\n",
      "Epoch [1/10], Batch [220], Loss: 3.4487\n",
      "Epoch [1/10], Batch [230], Loss: 3.4985\n",
      "Epoch [1/10], Batch [240], Loss: 3.4961\n",
      "Epoch [1/10], Batch [250], Loss: 3.5255\n",
      "Epoch [1/10], Batch [260], Loss: 3.5351\n",
      "Epoch [1/10], Batch [270], Loss: 3.5207\n",
      "Epoch [1/10], Batch [280], Loss: 3.5187\n",
      "Epoch [1/10], Batch [290], Loss: 3.4478\n",
      "Epoch [1/10], Batch [300], Loss: 3.4626\n",
      "Epoch [1/10], Batch [310], Loss: 3.4231\n",
      "Epoch [1/10], Batch [320], Loss: 3.4058\n",
      "Epoch [1/10], Batch [330], Loss: 3.4091\n",
      "Epoch [1/10], Batch [340], Loss: 3.4570\n",
      "Epoch [1/10], Batch [350], Loss: 3.4227\n",
      "Epoch [1/10], Batch [360], Loss: 3.4065\n",
      "Epoch [1/10], Batch [370], Loss: 3.3280\n",
      "Epoch [1/10], Batch [380], Loss: 3.3364\n",
      "Epoch [1/10], Batch [390], Loss: 3.3807\n",
      "Epoch [1/10], Batch [400], Loss: 3.3958\n",
      "Epoch [1/10], Batch [410], Loss: 3.4572\n",
      "Epoch [1/10], Batch [420], Loss: 3.4089\n",
      "Epoch [1/10], Batch [430], Loss: 3.4249\n",
      "Epoch [1/10], Batch [440], Loss: 3.4050\n",
      "Epoch [1/10], Batch [450], Loss: 3.3491\n",
      "Epoch [1/10], Batch [460], Loss: 3.2949\n",
      "Epoch [1/10], Batch [470], Loss: 3.4084\n",
      "Epoch [1/10], Batch [480], Loss: 3.3536\n",
      "Epoch [1/10], Batch [490], Loss: 3.4083\n",
      "Epoch [1/10], Batch [500], Loss: 3.2943\n",
      "Epoch [1/10], Batch [510], Loss: 3.3601\n",
      "Epoch [1/10], Batch [520], Loss: 3.3310\n",
      "Epoch [1/10], Batch [530], Loss: 3.4178\n",
      "Epoch [1/10], Batch [540], Loss: 3.3462\n",
      "Epoch [1/10], Batch [550], Loss: 3.3781\n",
      "Epoch [1/10], Batch [560], Loss: 3.3663\n",
      "Epoch [1/10], Batch [570], Loss: 3.4037\n",
      "Epoch [1/10], Batch [580], Loss: 3.3883\n",
      "Epoch [1/10], Batch [590], Loss: 3.3554\n",
      "Epoch [1/10], Batch [600], Loss: 3.4362\n",
      "Epoch [1/10], Batch [610], Loss: 3.4404\n",
      "Epoch [1/10], Batch [620], Loss: 3.3450\n",
      "Epoch [1/10], Batch [630], Loss: 3.3416\n",
      "Epoch [1/10], Batch [640], Loss: 3.2614\n",
      "Epoch [1/10], Batch [650], Loss: 3.3854\n",
      "Epoch [1/10], Batch [660], Loss: 3.2807\n",
      "Epoch [1/10], Batch [670], Loss: 3.3833\n",
      "Epoch [1/10], Batch [680], Loss: 3.2583\n",
      "Epoch [1/10], Batch [690], Loss: 3.4373\n",
      "Epoch [1/10], Batch [700], Loss: 3.3104\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mcpu(), target_tokens)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (input_tokens, target_tokens) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_tokens = input_tokens.to(device)\n",
    "        # Forward pass through the model\n",
    "        logits = model(input_tokens)\n",
    "\n",
    "        # Reshape logits and targets for loss computation\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        target_tokens = target_tokens.view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits.cpu(), target_tokens)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: VdqVh$RXltnusNZivewn\n"
     ]
    }
   ],
   "source": [
    "def generate_sequence(model, start_text, length=100):\n",
    "    model.eval()\n",
    "    input_tokens = torch.tensor([dataset.char_to_idx[c] for c in start_text]).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            logits = model(input_tokens)\n",
    "            logits = logits[:, -1, :]  # Get logits of the last token in the sequence\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_tokens = torch.cat([input_tokens, next_token], dim=1)\n",
    "\n",
    "    return ''.join([dataset.idx_to_char[idx.item()] for idx in input_tokens.squeeze()])\n",
    "\n",
    "# Generate text\n",
    "print(generate_sequence(model, start_text=\"ROMEO: \", length=20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
